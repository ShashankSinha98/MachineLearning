{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Interacting with GYM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01543768, -0.01070381, -0.03705663, -0.03993271])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "2\n",
      "Box(4,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Total no of actions available\n",
    "print(env.action_space)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample()) # Randomly select one action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "   \n",
    "for t in range(200):\n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action) # Perform action\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Playing multiple game episode with random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03840103 -0.23807675  0.02326431  0.33618221] 1.0 False {}\n",
      "[-0.04316257 -0.04329347  0.02998796  0.05092545] 1.0 False {}\n",
      "[-0.04402844  0.15138594  0.03100647 -0.23214723] 1.0 False {}\n",
      "[-0.04100072 -0.04416502  0.02636352  0.07015266] 1.0 False {}\n",
      "[-0.04188402  0.15056924  0.02776657 -0.21409725] 1.0 False {}\n",
      "[-0.03887264 -0.04493845  0.02348463  0.08721357] 1.0 False {}\n",
      "[-0.03977141  0.14983913  0.0252289  -0.1979684 ] 1.0 False {}\n",
      "[-0.03677462 -0.04563442  0.02126953  0.10256513] 1.0 False {}\n",
      "[-0.03768731 -0.24105463  0.02332084  0.40188191] 1.0 False {}\n",
      "[-0.0425084  -0.04627109  0.03135847  0.11664175] 1.0 False {}\n",
      "[-0.04343383 -0.24182799  0.03369131  0.41905077] 1.0 False {}\n",
      "[-0.04827038 -0.04719924  0.04207232  0.1371767 ] 1.0 False {}\n",
      "[-0.04921437 -0.24289775  0.04481586  0.44283032] 1.0 False {}\n",
      "[-0.05407232 -0.04843766  0.05367246  0.1646045 ] 1.0 False {}\n",
      "[-0.05504108  0.14587653  0.05696455 -0.11067492] 1.0 False {}\n",
      "[-0.05212355  0.3401379   0.05475106 -0.3848558 ] 1.0 False {}\n",
      "[-0.04532079  0.14428313  0.04705394 -0.0754251 ] 1.0 False {}\n",
      "[-0.04243513  0.33870004  0.04554544 -0.35289894] 1.0 False {}\n",
      "[-0.03566113  0.53314575  0.03848746 -0.6308793 ] 1.0 False {}\n",
      "[-0.02499821  0.72771014  0.02586987 -0.91119724] 1.0 False {}\n",
      "[-0.01044401  0.92247266  0.00764593 -1.19563838] 1.0 False {}\n",
      "[ 0.00800545  0.72725256 -0.01626684 -0.9005689 ] 1.0 False {}\n",
      "[ 0.0225505   0.92259111 -0.03427822 -1.1983202 ] 1.0 False {}\n",
      "[ 0.04100232  0.72792909 -0.05824462 -0.91657452] 1.0 False {}\n",
      "[ 0.0555609   0.92378819 -0.07657611 -1.22697922] 1.0 False {}\n",
      "[ 0.07403666  1.11980776 -0.1011157  -1.5426383 ] 1.0 False {}\n",
      "[ 0.09643282  0.92603621 -0.13196846 -1.28314298] 1.0 False {}\n",
      "[ 0.11495354  1.12256852 -0.15763132 -1.61406433] 1.0 False {}\n",
      "[ 0.13740491  0.92961889 -0.18991261 -1.37438013] 1.0 False {}\n",
      "[ 0.15599729  0.73730911 -0.21740021 -1.14660112] 1.0 True {}\n",
      "Game Episode 1/2, Score: 29\n",
      "All 20 episode over!\n"
     ]
    }
   ],
   "source": [
    "# Playing 2 game episode\n",
    "for e in range(1):\n",
    "    \n",
    "    observation = env.reset()\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        random_action = env.action_space.sample()\n",
    "        observation,reward,done,other_info = env.step(random_action) # Observation = new state\n",
    "        print(observation,reward,done,other_info)\n",
    "        if done:\n",
    "            # Game episode over\n",
    "            print(\"Game Episode %d/2, Score: %d\"%(e+1,t))\n",
    "            break\n",
    "            \n",
    "env.close()\n",
    "print(\"All 20 episode over!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q Learning\n",
    "- Agent Design & Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size # 4\n",
    "        self.action_size = action_size # 2 - left or right\n",
    "        self.memory = deque(maxLen=2000)\n",
    "        self.gamma = 0.95 # Discount factor\n",
    "        # Exploration vs Exploitation tradeoff\n",
    "        # Exploration: good in beginning - helps you to try various random things\n",
    "        # Explotation: good in end - sample good experiences from the past(memory)\n",
    "        self.epsilon = 1.0 # 100% random exploration in beginning\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learninng_rate = 0.001\n",
    "        self.model = self._create_model() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def _create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        # Remember Past experience\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self,state):\n",
    "        # Sampling according to Greedy Epsion method\n",
    "        if np.random.rand()<=self.epsilon():\n",
    "            # Take a random action\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Ask NN to give me the suitable action\n",
    "        return np.argmax(model.predict(state)[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, batch_size=32):\n",
    "        # Training using a 'Replay Buffer'\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        \n",
    "        for experience in minibatch:\n",
    "            state,action,reward,next_state,done = experience\n",
    "            \n",
    "            if not done:\n",
    "                # Game is not over, belman eqn to approx the target value of reward\n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "                \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "                \n",
    "            # X = state, Y = target_f\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            \n",
    "            \n",
    "    def load(self,name):\n",
    "        self.model.load_weightsd_weights(name)\n",
    "                                         \n",
    "                                         \n",
    "        \n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
