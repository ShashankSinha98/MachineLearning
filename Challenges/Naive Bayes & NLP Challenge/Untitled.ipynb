{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv('Train.csv')\n",
    "Test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.head(n=5)\n",
    "Train = Train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Carell comes into his own in his first starring role in the 40 Year Old Virgin, having only had supporting roles in such films as Bewitched, Bruce Almighty, Anchorman, and his work on the Daily Show, we had only gotten a small taste of the comedy that Carell truly makes his own. You can tell that Will Ferrell influenced his \"comedic air\" but Carell takes it to another level, everything he does is innocent, lovable, and hilarious. I would not hesitate to say that Steve Carell is one of the next great comedians of our time.<br /><br />The 40 Year Old Virgin is two hours of non-stop laughs (or 4 hours if you see it twice like I did), a perfect supporting cast and great leads charm the audience through the entire movie. The script was perfect with so many great lines that you will want to see the movie again just to try to remember them all. The music fit the tone of the movie great, and you can tell the director knew what he was doing.<br /><br />Filled with sex jokes, some nudity, and a lot of language, this movie isn't for everyone but if you liked the Wedding Crashers, Anchorman, or any movie along those lines, you will absolutely love The 40 Year Old Virgin.\n",
      "(40000,)\n",
      "pos\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "X_Train = Train[:,0]\n",
    "Y_Train = Train[:,-1]\n",
    "print(X_Train[5])\n",
    "print(X_Train.shape)\n",
    "print(Y_Train[5])\n",
    "print(Y_Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replacedd\n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
    "    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n",
    "    \n",
    "    return cleaned_str # returning the preprocessed string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes # Constructor is sinply passed with unique number of classes of the training sett\n",
    "        \n",
    "        \n",
    "    def addToBow(self,example,dict_index):\n",
    "        \n",
    "        #print(\"Ex 1: \",example)\n",
    "        \n",
    "        if isinstance(example,np.ndarray):\n",
    "            example=example[0]\n",
    "            #print(\"is instance executed\")\n",
    "        \n",
    "        #print(\"Ex 2: \",example)\n",
    "        #print(\"dict indx:\",dict_index)\n",
    "        \n",
    "        for token_word in example.split(): #for every word in preprocessed example\n",
    "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count\n",
    "            \n",
    "            \n",
    "    def train(self,dataset,labels):\n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        print(\"Init Bow Dict\",self.bow_dicts)\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
    "            \n",
    "        #constructing BoW for each category\n",
    "        #print(self.labels==0)\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            all_cat_examples=self.examples[self.labels==cat]\n",
    "            \n",
    "        \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            #print(\"Cleaned Ex 1: \",cleaned_examples)\n",
    "            #print(\"Cleaned Ex 1 type: \",type(cleaned_examples))\n",
    "        \n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            #print(\"Cleaned Ex 2: \",cleaned_examples)\n",
    "            #print(\"Cleaned Ex 2 type: \",type(cleaned_examples))\n",
    "            \n",
    "            #now costruct BoW of this particular category\n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "            \n",
    "            prob_classes=np.empty(self.classes.shape[0])\n",
    "            all_words=[]\n",
    "            cat_word_counts=np.empty(self.classes.shape[0])\n",
    "            \n",
    "            for cat_index,cat in enumerate(self.classes):\n",
    "                #Calculating prior probability p(c) for each class\n",
    "                prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    "            \n",
    "                #Calculating total counts of all the words of each class \n",
    "                count=list(self.bow_dicts[cat_index].values())\n",
    "                cat_word_counts[cat_index]=np.sum(np.array(count))+1 # |v| is remaining to be added\n",
    "            \n",
    "                #get all words of this category                                \n",
    "                all_words+=self.bow_dicts[cat_index].keys()\n",
    "                \n",
    "            #combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "        \n",
    "            self.vocab=np.unique(np.array(all_words))\n",
    "            self.vocab_length=self.vocab.shape[0]\n",
    "            \n",
    "            #computing denominator value                                      \n",
    "            denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Now that we have everything precomputed as well, its better to organize everything in a tuple \n",
    "            rather than to have a separate list for every thing.\n",
    "            \n",
    "            Every element of self.cats_info has a tuple of values\n",
    "            Each tuple has a dict at index 0, prior probability at index 1, denominator value at index 2\n",
    "            '''\n",
    "        \n",
    "            self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "            self.cats_info=np.array(self.cats_info)\n",
    "            \n",
    "            \n",
    "    def getExampleProb(self,test_example):\n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each clasS\n",
    "        \n",
    "        #finding probability w.r.t each class of the given test example\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            \n",
    "            for test_token in test_example.split(): #split the test example and get p of each test word\n",
    "                \n",
    "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                \n",
    "                #now get likelihood of this test_token word                              \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])\n",
    "                \n",
    "                #remember why taking log? To prevent underflow!\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                \n",
    "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])\n",
    "            \n",
    "        return post_prob\n",
    "    \n",
    "    \n",
    "    def test(self,test_set):\n",
    "        \n",
    "        predictions=[] #to store prediction of each test example\n",
    "        for example in test_set: \n",
    "                                              \n",
    "            #preprocess the test example the same way we did for training set exampels                                  \n",
    "            cleaned_example=preprocess_string(example) \n",
    "             \n",
    "            #simply get the posterior probability of every example                                  \n",
    "            post_prob=self.getExampleProb(cleaned_example) #get prob of this example for both classes\n",
    "            \n",
    "            #simply pick the max value and map against self.classes!\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions) \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=NaiveBayes(np.unique(Y_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Bow Dict [defaultdict(<function NaiveBayes.train.<locals>.<listcomp>.<lambda> at 0x00000104F2ED0310>, {})\n",
      " defaultdict(<function NaiveBayes.train.<locals>.<listcomp>.<lambda> at 0x00000104F2ED01F0>, {})]\n"
     ]
    }
   ],
   "source": [
    "nb.train(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember those old kung fu movies we used to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is another one on my List of Movies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How in the world does a thing like this get in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Queen of the Damned\" is one of the best vampi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Caprica episode (S01E01) is well done as a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  Remember those old kung fu movies we used to w...\n",
       "1  This movie is another one on my List of Movies...\n",
       "2  How in the world does a thing like this get in...\n",
       "3  \"Queen of the Damned\" is one of the best vampi...\n",
       "4  The Caprica episode (S01E01) is well done as a..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "Test = Test.values\n",
    "X_Test = Test[:,0]\n",
    "print(X_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Pred = nb.test(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'neg' 'neg' 'pos' 'pos']\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_Pred[:5])\n",
    "print(Y_Pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving File\n",
    "df = pd.DataFrame(data=Y_Pred,columns=[\"label\"])\n",
    "df.to_csv(\"Movie_Y_Pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_Pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy on training data by splitting it\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_Train, Y_Train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670,) (670,)\n",
      "(330,) (330,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Bow Dict [defaultdict(<function NaiveBayes.train.<locals>.<listcomp>.<lambda> at 0x00000104E4B82820>, {})\n",
      " defaultdict(<function NaiveBayes.train.<locals>.<listcomp>.<lambda> at 0x00000104DCAE4160>, {})]\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes(np.unique(y_train))\n",
    "nb.train(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'pos' 'neg' 'pos' 'neg']\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.test(x_test)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8272727272727273\n"
     ]
    }
   ],
   "source": [
    "acc = np.sum(y_pred==y_test)/y_test.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "import clean_review  as cr\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean = [cr.getCleanReview(i) for i in x_train]\n",
    "xt_clean = [cr.getCleanReview(i) for i in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watch show cousin hate first girl dress style cloth first letter name come could better villain spare first monkey part littl brain show gay version devil pink hillbilli gang green gang whit iron name spoil princess iron name among other also found male hero show sexist anyth rather watch sailor moon much better someon els want watch show room find way break televis believ save half hour tortur rate give deserv whatev watch', 'twelv monkey got element becom terri gilliam masterpiec outstand screenplay sustain rhythm clever sometim iron dialog moreov good nose cast twelv monkey also first movi bruce willi stand back kind charact use play previou movi jade hopeless charact could nicknam prison took fearless invinc hero case die hard matter tri prison time movi contain thrill end got real dramat power terrif movi also reflect man danger dread notabl one could caus end world viru creat ill matter long take twelv monkey estim true valu one masterpiec made nineti', 'white chick hold dress black chick oh yeah look differ anyon give one wayan movi dress ladi menac count jack white michael costanza ghost wrote norton trio member act director white chick never realli joke wayan act like girl hour setup punchlin laugh lot think gonna play time crisi least time exact somebodi tell kenan ivori damon marlon shawn damien talent one kim rakeesha georg w osama bin wayan stop make movi hurt zone layer verdict', 'rocket govern experi effect cosmic ray anim crash small texa town peopl start die counti sheriff tri investig hamper effort govern offici turn mutant space gorilla loos kill teenag wood like low budget scienc fiction horror movi like monster movi thought would good chanc would like movi sadli mind bad act corni dialog atroci music score giant plot hole movi lot movi problem seen enjoy bad good kind way other type night fright differ night fright terribl pace drag scene go without anyth happen search wood clue peopl walk forest long time sever seemingli endless danc teen parti wood noth interest go scene shorter movi might bore though think simpli cut scene would save one given movi three view make sure gave chanc slam review sadli gotten wors watch fourth', 'terrif fast pace screwbal like comic strip drama farc set franc implos play wide eye straight face intens talent cast chockablock action satir social commentari authent period detail slick brillantin hairdo marcel hairdo fleet citroen traction rollick soundtrack brief credibl imperson charl de gaull marshal petain simpli best entertain recent shown screen devoid presumpt messag movi train creativ recreat trip steam driven train work despit steam locomot expens prop doubt would tgv']\n"
     ]
    }
   ],
   "source": [
    "print(x_clean[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 10097)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "# Vectorization on train set\n",
    "x_vec = cv.fit_transform(x_clean).toarray()\n",
    "xt_vec = cv.transform(xt_clean).toarray()\n",
    "print(x_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(x_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8393939393939394"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(xt_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(x_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8393939393939394"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb.score(xt_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
